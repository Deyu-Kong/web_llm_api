# main.py
"""
Pantheon API v0.1
é€šè¿‡ DrissionPage æ§åˆ¶æµè§ˆå™¨ï¼Œå°†ç½‘é¡µç‰ˆ LLM å°è£…ä¸º API
æ”¯æŒ OpenAI å…¼å®¹æ ¼å¼
"""

import uvicorn
import time
import uuid
from fastapi import FastAPI, HTTPException, Header
from pydantic import BaseModel, Field
from typing import Optional, List, Union, Literal
from DrissionPage import ChromiumPage, ChromiumOptions
import sys

from config import CHROME_PORT, CHROME_USER_DATA_DIR, DEFAULT_LMARENA_MODEL
from adapters import KimiBot, LMArenaBot

# ============== FastAPI åˆå§‹åŒ– ==============
app = FastAPI(
    title="Pantheon API",
    description="å°†ç½‘é¡µç‰ˆ LLM å°è£…ä¸ºæœ¬åœ° APIï¼ˆæ”¯æŒ OpenAI å…¼å®¹æ ¼å¼ï¼‰",
    version="0.1.0"
)

# ============== å…¨å±€å˜é‡ ==============
browser = None
kimi_bot = None
lmarena_bot = None

# ============== OpenAI å…¼å®¹æ•°æ®æ¨¡å‹ ==============

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = Field(default=1.0, ge=0, le=2)
    top_p: Optional[float] = Field(default=1.0, ge=0, le=1)
    n: Optional[int] = Field(default=1, ge=1)
    stream: Optional[bool] = False
    stop: Optional[Union[str, List[str]]] = None
    max_tokens: Optional[int] = None
    presence_penalty: Optional[float] = Field(default=0, ge=-2, le=2)
    frequency_penalty: Optional[float] = Field(default=0, ge=-2, le=2)
    user: Optional[str] = None

class ChatCompletionChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: Literal["stop", "length", "content_filter", "null"]

class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str
    object: Literal["chat.completion"] = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Usage

class ModelInfo(BaseModel):
    id: str
    object: Literal["model"] = "model"
    created: int
    owned_by: str

class ModelListResponse(BaseModel):
    object: Literal["list"] = "list"
    data: List[ModelInfo]

# ============== åŸæœ‰æ•°æ®æ¨¡å‹ ==============

class ChatRequest(BaseModel):
    query: str
    new_chat: Optional[bool] = False  # æ˜¯å¦å¼€å¯æ–°å¯¹è¯

class LMArenaRequest(BaseModel):
    query: str
    model: Optional[str] = None  # æŒ‡å®šæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    new_chat: Optional[bool] = False

class ChatResponse(BaseModel):
    model: str
    answer: str
    status: str
    
class LMArenaResponse(BaseModel):
    model: str
    thought: str  # æ€è€ƒè¿‡ç¨‹
    answer: str   # å®é™…å›ç­”
    status: str

# ============== æ¨¡å‹è·¯ç”±é€»è¾‘ ==============

def parse_model_name(model: str) -> tuple:
    """
    è§£ææ¨¡å‹åç§°ï¼Œè¿”å› (bot_type, specific_model)
    
    æ”¯æŒçš„æ ¼å¼:
    - "kimi" -> ("kimi", None)
    - "lmarena" -> ("lmarena", None)
    - "lmarena:claude-opus-4" -> ("lmarena", "claude-opus-4")
    - "lmarena:gemini-3-pro" -> ("lmarena", "gemini-3-pro")
    """
    model = model.lower().strip()
    
    if model.startswith("lmarena:"):
        parts = model.split(":", 1)
        return ("lmarena", parts[1] if len(parts) > 1 else None)
    elif model.startswith("lmarena"):
        return ("lmarena", None)
    elif model.startswith("kimi"):
        return ("kimi", None)
    else:
        # é»˜è®¤å°è¯•ä½œä¸º lmarena çš„æ¨¡å‹å
        return ("lmarena", model)

def route_to_bot(model: str, query: str, new_chat: bool = False) -> dict:
    """
    æ ¹æ®æ¨¡å‹åç§°è·¯ç”±åˆ°å¯¹åº”çš„ bot
    è¿”å›: {"model": str, "thought": str, "answer": str}
    """
    global kimi_bot, lmarena_bot
    
    bot_type, specific_model = parse_model_name(model)
    print(f"[Router] è§£ææ¨¡å‹: {model} -> bot_type={bot_type}, specific_model={specific_model}")
    
    if bot_type == "kimi":
        if kimi_bot is None:
            raise HTTPException(status_code=503, detail="Kimi æœºå™¨äººæœªåˆå§‹åŒ–")
        
        if new_chat:
            kimi_bot.new_chat()
        
        kimi_bot.activate()
        answer = kimi_bot.ask(query)
        
        if answer.startswith("Error:"):
            raise HTTPException(status_code=500, detail=answer)
        
        return {
            "model": "kimi",
            "thought": "",
            "answer": answer
        }
    
    elif bot_type == "lmarena":
        if lmarena_bot is None:
            raise HTTPException(status_code=503, detail="LMArena æœºå™¨äººæœªåˆå§‹åŒ–")
        
        if new_chat:
            lmarena_bot.new_chat()
        
        lmarena_bot.activate()
        result = lmarena_bot.ask(query, model_name=specific_model)
        
        if result["answer"].startswith("Error:"):
            raise HTTPException(status_code=500, detail=result["answer"])
        
        return {
            "model": f"lmarena:{specific_model}" if specific_model else "lmarena",
            "thought": result.get("thought", ""),
            "answer": result["answer"]
        }
    
    else:
        raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}")

# ============== å¯åŠ¨äº‹ä»¶ ==============
@app.on_event("startup")
def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶è¿æ¥æµè§ˆå™¨"""
    global browser, kimi_bot, lmarena_bot
    
    print("=" * 50)
    print("ğŸš€ Pantheon API v0.1 å¯åŠ¨ä¸­...")
    print("   æ”¯æŒ OpenAI å…¼å®¹æ ¼å¼")
    print("=" * 50)
    
    try:
        # é…ç½® Chrome é€‰é¡¹
        co = ChromiumOptions()
        
        # è®¾ç½®è¿œç¨‹è°ƒè¯•ç«¯å£
        co.set_local_port(CHROME_PORT)
                
        # å…¶ä»–æœ‰ç”¨çš„é…ç½®
        co.set_argument('--no-sandbox')  # ç¦ç”¨æ²™ç›’æ¨¡å¼
        co.set_argument('--disable-gpu')  # ç¦ç”¨ GPU åŠ é€Ÿï¼ˆå¯é€‰ï¼‰
        # co.headless(False)  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼ˆé»˜è®¤ï¼‰
        
        # è¿æ¥æˆ–å¯åŠ¨æµè§ˆå™¨
        print(f"ğŸ”Œ å°è¯•è¿æ¥åˆ°ç«¯å£ {CHROME_PORT}...")
        browser = ChromiumPage(addr_or_opts=co)
        
        print(f"âœ… æˆåŠŸè¿æ¥åˆ° Chrome (ç«¯å£: {CHROME_PORT})")
        
        # åˆå§‹åŒ– Kimi æœºå™¨äºº
        kimi_bot = KimiBot(browser)
        print("âœ… Kimi æœºå™¨äººåˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ– LMArena æœºå™¨äºº
        lmarena_bot = LMArenaBot(browser, model_name=DEFAULT_LMARENA_MODEL)
        print("âœ… LMArena æœºå™¨äººåˆå§‹åŒ–å®Œæˆ")
        
        print("\n" + "=" * 50)
        print("âœ… æœåŠ¡å¯åŠ¨æˆåŠŸï¼")
        print("=" * 50)
        print("\nğŸ“Œ OpenAI å…¼å®¹ç«¯ç‚¹:")
        print("   POST /v1/chat/completions")
        print("   GET  /v1/models")
        print("\nğŸ“Œ æ¨¡å‹åç§°æ ¼å¼:")
        print("   - kimi")
        print("   - lmarena")
        print("   - lmarena:claude-opus-4-5-20251101-thinking-32k")
        print("   - lmarena:gemini-3-pro")
        print("=" * 50 + "\n")
        
    except Exception as e:
        print(f"\nâŒ å¯åŠ¨å¤±è´¥: {e}")
        print("\n" + "=" * 50)
        print("ğŸ“Œ è§£å†³æ–¹æ³•ï¼š")
        print("=" * 50)
        print(f"\næ–¹æ¡ˆ: æ‰‹åŠ¨å¯åŠ¨ Chrome åå†è¿è¡Œç¨‹åº")
        print(f'   chrome.exe --remote-debugging-port={CHROME_PORT} --user-data-dir="{CHROME_USER_DATA_DIR}"')
        print("=" * 50 + "\n")
        sys.exit(1)

# ============== OpenAI å…¼å®¹ API ==============

@app.get("/v1/models", response_model=ModelListResponse)
def list_models():
    """
    åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹ï¼ˆOpenAI å…¼å®¹ï¼‰
    """
    current_time = int(time.time())
    
    models = [
        ModelInfo(id="kimi", created=current_time, owned_by="moonshot"),
        ModelInfo(id="lmarena", created=current_time, owned_by="lmarena"),
        ModelInfo(id="lmarena:claude-opus-4-5-20251101-thinking-32k", created=current_time, owned_by="lmarena"),
        ModelInfo(id="lmarena:gpt-4o", created=current_time, owned_by="lmarena"),
        ModelInfo(id="lmarena:gemini-2.5-pro", created=current_time, owned_by="lmarena"),
    ]
    
    return ModelListResponse(data=models)

@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
def chat_completions(
    request: ChatCompletionRequest,
    authorization: Optional[str] = Header(None)
):
    """
    åˆ›å»ºèŠå¤©è¡¥å…¨ï¼ˆOpenAI å…¼å®¹ï¼‰
    
    æ”¯æŒçš„æ¨¡å‹æ ¼å¼:
    - kimi: ä½¿ç”¨ Kimi
    - lmarena: ä½¿ç”¨ LMArenaï¼ˆé»˜è®¤æ¨¡å‹ï¼‰
    - lmarena:æ¨¡å‹å: ä½¿ç”¨ LMArena æŒ‡å®šæ¨¡å‹
    
    ç¤ºä¾‹:
    - model: "kimi"
    - model: "lmarena"
    - model: "lmarena:claude-opus-4-5-20251101-thinking-32k"
    - model: "lmarena:gemini-3-pro"
    """
    
    # æ£€æŸ¥æ˜¯å¦è¯·æ±‚æµå¼è¾“å‡ºï¼ˆå½“å‰ä¸æ”¯æŒï¼‰
    if request.stream:
        raise HTTPException(
            status_code=400, 
            detail="å½“å‰ç‰ˆæœ¬ä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œè¯·è®¾ç½® stream=false"
        )
    
    # æå–ç”¨æˆ·æ¶ˆæ¯
    # åˆå¹¶æ‰€æœ‰æ¶ˆæ¯ä¸ºä¸€ä¸ªæŸ¥è¯¢ï¼ˆç®€åŒ–å¤„ç†ï¼‰
    messages = request.messages
    
    # æ„å»ºæŸ¥è¯¢æ–‡æœ¬
    query_parts = []
    for msg in messages:
        if msg.role == "system":
            query_parts.append(f"[ç³»ç»ŸæŒ‡ä»¤] {msg.content}")
        elif msg.role == "user":
            query_parts.append(msg.content)
        elif msg.role == "assistant":
            # åŠ©æ‰‹æ¶ˆæ¯å¯ä»¥ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆä½†å½“å‰å®ç°æ˜¯æ–°å¯¹è¯ï¼Œæ‰€ä»¥è·³è¿‡ï¼‰
            pass
    
    query = "\n".join(query_parts)
    
    if not query.strip():
        raise HTTPException(status_code=400, detail="æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º")
    
    print(f"[OpenAI API] æ”¶åˆ°è¯·æ±‚:")
    print(f"  æ¨¡å‹: {request.model}")
    print(f"  æ¶ˆæ¯æ•°: {len(messages)}")
    print(f"  æŸ¥è¯¢: {query[:100]}...")
    
    # è·¯ç”±åˆ°å¯¹åº”çš„ bot
    # æ³¨æ„ï¼šè¿™é‡Œæ¯æ¬¡è¯·æ±‚éƒ½å¼€å¯æ–°å¯¹è¯ï¼Œå› ä¸º OpenAI API æ˜¯æ— çŠ¶æ€çš„
    # å¦‚æœéœ€è¦æ”¯æŒå¤šè½®å¯¹è¯ï¼Œéœ€è¦é¢å¤–çš„ä¼šè¯ç®¡ç†
    result = route_to_bot(request.model, query, new_chat=True)
    
    # æ„å»ºå›å¤å†…å®¹
    # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹ï¼Œå¯ä»¥é€‰æ‹©æ€§åœ°åŒ…å«åœ¨å›å¤ä¸­
    answer_content = result["answer"]
    if result.get("thought"):
        # å¯é€‰ï¼šå°†æ€è€ƒè¿‡ç¨‹ä½œä¸ºæ³¨é‡ŠåŒ…å«
        # answer_content = f"<æ€è€ƒ>\n{result['thought']}\n</æ€è€ƒ>\n\n{result['answer']}"
        pass
    
    # ä¼°ç®— token æ•°ï¼ˆç®€å•æŒ‰å­—ç¬¦ä¼°ç®—ï¼Œå®é™…åº”è¯¥ç”¨ tiktokenï¼‰
    prompt_tokens = len(query) // 2  # ç²—ç•¥ä¼°è®¡
    completion_tokens = len(answer_content) // 2
    
    # æ„å»ºå“åº”
    response = ChatCompletionResponse(
        id=f"chatcmpl-{uuid.uuid4().hex[:12]}",
        created=int(time.time()),
        model=result["model"],
        choices=[
            ChatCompletionChoice(
                index=0,
                message=ChatMessage(
                    role="assistant",
                    content=answer_content
                ),
                finish_reason="stop"
            )
        ],
        usage=Usage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=prompt_tokens + completion_tokens
        )
    )
    
    print(f"[OpenAI API] å“åº”å®Œæˆï¼Œå›å¤é•¿åº¦: {len(answer_content)} å­—ç¬¦")
    
    return response

# ============== åŸæœ‰ APIï¼ˆä¿ç•™å…¼å®¹ï¼‰ ==============

@app.get("/")
def root():
    """æ ¹è·¯å¾„ - çŠ¶æ€æ£€æŸ¥"""
    return {
        "status": "running",
        "version": "0.1.0",
        "available_models": ["kimi", "lmarena", "lmarena:<model_name>"],
        "openai_compatible": True,
        "endpoints": {
            "openai": "/v1/chat/completions",
            "models": "/v1/models",
            "kimi": "/v1/chat/kimi",
            "lmarena": "/v1/chat/lmarena"
        }
    }

@app.get("/health")
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "browser_connected": browser is not None}

@app.post("/v1/chat/kimi", response_model=ChatResponse)
def chat_with_kimi(request: ChatRequest):
    """
    ä¸ Kimi å¯¹è¯
    
    - **query**: ç”¨æˆ·é—®é¢˜
    - **new_chat**: æ˜¯å¦å¼€å¯æ–°å¯¹è¯ (å¯é€‰ï¼Œé»˜è®¤ False)
    """
    global kimi_bot
    
    if kimi_bot is None:
        raise HTTPException(status_code=503, detail="Kimi æœºå™¨äººæœªåˆå§‹åŒ–")
    
    try:
        # æ˜¯å¦éœ€è¦å¼€å¯æ–°å¯¹è¯
        if request.new_chat:
            kimi_bot.new_chat()
        
        # æ¿€æ´»æ ‡ç­¾é¡µ
        kimi_bot.activate()
        
        # å‘é€é—®é¢˜å¹¶è·å–å›ç­”
        answer = kimi_bot.ask(request.query)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if answer.startswith("Error:"):
            raise HTTPException(status_code=500, detail=answer)
        
        return ChatResponse(
            model="kimi-web",
            answer=answer,
            status="success"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ä¿®æ”¹ LMArena è·¯ç”±
@app.post("/v1/chat/lmarena", response_model=LMArenaResponse)
def chat_with_lmarena(request: LMArenaRequest):
    """
    ä¸ LMArena å¯¹è¯
    
    - **query**: ç”¨æˆ·é—®é¢˜
    - **model**: æŒ‡å®šæ¨¡å‹åç§° (å¯é€‰)
    - **new_chat**: æ˜¯å¦å¼€å¯æ–°å¯¹è¯ (å¯é€‰ï¼Œé»˜è®¤ False)
    """
    global lmarena_bot
    
    if lmarena_bot is None:
        raise HTTPException(status_code=503, detail="LMArena æœºå™¨äººæœªåˆå§‹åŒ–")
    
    try:
        # æ˜¯å¦éœ€è¦å¼€å¯æ–°å¯¹è¯
        if request.new_chat:
            lmarena_bot.new_chat()
        
        # æ¿€æ´»æ ‡ç­¾é¡µ
        lmarena_bot.activate()
        
        # å‘é€é—®é¢˜å¹¶è·å–å›ç­”ï¼ˆå¯æŒ‡å®šæ¨¡å‹ï¼‰
        result = lmarena_bot.ask(request.query, model_name=request.model)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if result["answer"].startswith("Error:"):
            raise HTTPException(status_code=500, detail=result["answer"])
        
        # è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹
        current_model = lmarena_bot.current_model or "default"
        
        return LMArenaResponse(
            model=f"lmarena-{current_model}",
            thought=result["thought"],
            answer=result["answer"],
            status="success"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============== ä¸»å…¥å£ ==============
if __name__ == "__main__":
    print("\nğŸ“Œ ä½¿ç”¨è¯´æ˜:")
    print("=" * 50)
    print("1. é¦–æ¬¡ä½¿ç”¨è¯·å…ˆä¿®æ”¹ config.py ä¸­çš„ç”¨æˆ·æ•°æ®ç›®å½•")
    print(f"   å½“å‰: {CHROME_USER_DATA_DIR}")
    print("2. ç¡®ä¿å·²åœ¨æµè§ˆå™¨ä¸­ç™»å½•å¯¹åº”å¹³å°")
    print("3. API æ–‡æ¡£: http://127.0.0.1:8000/docs")
    print("\nğŸ“Œ OpenAI å…¼å®¹è°ƒç”¨ç¤ºä¾‹:")
    print('   from openai import OpenAI')
    print('   client = OpenAI(base_url="http://127.0.0.1:8000/v1", api_key="not-needed")')
    print('   response = client.chat.completions.create(')
    print('       model="lmarena:claude-opus-4",')
    print('       messages=[{"role": "user", "content": "Hello!"}]')
    print('   )')
    print("=" * 50 + "\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)