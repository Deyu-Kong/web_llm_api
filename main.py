# main.py
"""
WebLLM API v0.5
é€šè¿‡ DrissionPage æ§åˆ¶æµè§ˆå™¨ï¼Œå°†ç½‘é¡µç‰ˆ LLM å°è£…ä¸º OpenAI å…¼å®¹ API
æ”¯æŒå¤šæ ‡ç­¾é¡µå¹¶è¡Œå¤„ç†è¯·æ±‚
"""

import uvicorn
import time
import uuid
import sys
import threading
from concurrent.futures import ThreadPoolExecutor
from fastapi import FastAPI, HTTPException, Header, BackgroundTasks
from pydantic import BaseModel, Field
from typing import Optional, List, Literal
from DrissionPage import ChromiumPage, ChromiumOptions

from config import CHROME_PORT, CHROME_USER_DATA_DIR, DEFAULT_LMARENA_MODEL
from adapters import KimiBot, LMArenaBot, YuanbaoBot, DeepSeekBot, BaseBot
from core import TabPoolManager

# ============== FastAPI åˆå§‹åŒ– ==============
app = FastAPI(
    title="WebLLM API",
    description="å°†ç½‘é¡µç‰ˆ LLM å°è£…ä¸º OpenAI å…¼å®¹ API",
    version="0.5.0"
)

# ============== å…¨å±€å˜é‡ ==============
browser = None
tab_pool: TabPoolManager = None
executor = ThreadPoolExecutor(max_workers=10)

# Bot ç±»æ˜ å°„
BOT_CLASSES = {
    "kimi": KimiBot,
    "deepseek": DeepSeekBot,
    "yuanbao": YuanbaoBot,
    "lmarena": LMArenaBot,
}

# ============== æ•°æ®æ¨¡å‹ ==============

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = Field(default=1.0, ge=0, le=2)
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False

class ChatCompletionChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: Literal["stop", "length"]

class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str
    object: Literal["chat.completion"] = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Usage

class ModelInfo(BaseModel):
    id: str
    object: Literal["model"] = "model"
    created: int
    owned_by: str

class ModelListResponse(BaseModel):
    object: Literal["list"] = "list"
    data: List[ModelInfo]

# ============== æ¨¡å‹è·¯ç”± ==============

# æ”¯æŒçš„æ¨¡å‹æ˜ å°„
MODEL_ALIASES = {
    "kimi": ("kimi", None),
    "deepseek": ("deepseek", None),
    "ds": ("deepseek", None),
    "yuanbao": ("yuanbao", None),
    "tencent": ("yuanbao", None),
    "lmarena": ("lmarena", None),
}

def parse_model_name(model: str) -> tuple:
    """è§£ææ¨¡å‹åç§° -> (bot_type, specific_model)"""
    model = model.lower().strip()
    
    # æ£€æŸ¥ lmarena:xxx æ ¼å¼
    if model.startswith("lmarena:"):
        return ("lmarena", model.split(":", 1)[1])
    
    # æŸ¥æ‰¾åˆ«å
    if model in MODEL_ALIASES:
        return MODEL_ALIASES[model]
    
    # é»˜è®¤ä½œä¸º lmarena æ¨¡å‹
    return ("lmarena", model)


def create_bot_instance(bot_type: str, tab) -> BaseBot:
    """ä¸ºæŒ‡å®šæ ‡ç­¾é¡µåˆ›å»º Bot å®ä¾‹"""
    bot_class = BOT_CLASSES.get(bot_type)
    if not bot_class:
        raise ValueError(f"æœªçŸ¥çš„ Bot ç±»å‹: {bot_type}")
    
    # åˆ›å»º Bot å®ä¾‹ï¼Œä¼ å…¥ tab
    if bot_type == "lmarena":
        bot = bot_class(page=None, tab=tab, model_name=DEFAULT_LMARENA_MODEL)
    else:
        bot = bot_class(page=None, tab=tab)
    
    return bot


def execute_chat(bot_type: str, query: str, specific_model: str = None) -> dict:
    """
    åœ¨ç‹¬ç«‹æ ‡ç­¾é¡µä¸­æ‰§è¡Œå¯¹è¯
    
    è¿™æ˜¯æ ¸å¿ƒå‡½æ•°ï¼šä»æ ‡ç­¾é¡µæ± è·å–æ ‡ç­¾é¡µï¼Œåˆ›å»º Botï¼Œæ‰§è¡Œå¯¹è¯
    """
    global tab_pool
    
    request_id = uuid.uuid4().hex[:8]
    print(f"[{request_id}] å¼€å§‹å¤„ç†: {bot_type}, æŸ¥è¯¢: {query[:30]}...")
    
    # ä»æ± ä¸­è·å–æ ‡ç­¾é¡µ
    with tab_pool.get_tab(bot_type) as tab_info:
        try:
            # åˆ›å»º Bot å®ä¾‹
            bot = create_bot_instance(bot_type, tab_info.tab)
            
            # æ¿€æ´»å¹¶å¼€æ–°å¯¹è¯
            bot.activate()
            bot.new_chat()
            
            # æ‰§è¡Œå¯¹è¯
            if bot_type == "kimi":
                answer = bot.ask(query)
                result = {"thought": "", "answer": answer}
            elif bot_type == "lmarena":
                result = bot.ask(query, model_name=specific_model)
            else:
                result = bot.ask(query)
            
            # æ£€æŸ¥é”™è¯¯
            answer = result if isinstance(result, str) else result.get("answer", "")
            if answer.startswith("Error:"):
                raise Exception(answer)
            
            print(f"[{request_id}] âœ… å®Œæˆ")
            
            return {
                "model": f"{bot_type}:{specific_model}" if specific_model else bot_type,
                "thought": result.get("thought", "") if isinstance(result, dict) else "",
                "answer": answer if isinstance(result, str) else result.get("answer", ""),
                "query": query
            }
            
        except Exception as e:
            print(f"[{request_id}] âŒ å¤±è´¥: {e}")
            raise


def build_query(messages: List[ChatMessage]) -> str:
    """æ„å»ºæŸ¥è¯¢æ–‡æœ¬"""
    parts = []
    for msg in messages:
        if msg.role == "system":
            parts.append(f"[ç³»ç»ŸæŒ‡ä»¤] {msg.content}")
        elif msg.role == "user":
            parts.append(msg.content)
    return "\n".join(parts)


def build_response(result: dict) -> ChatCompletionResponse:
    """æ„å»º OpenAI æ ¼å¼å“åº”"""
    answer = result["answer"]
    query = result.get("query", "")
    
    return ChatCompletionResponse(
        id=f"chatcmpl-{uuid.uuid4().hex[:12]}",
        created=int(time.time()),
        model=result["model"],
        choices=[
            ChatCompletionChoice(
                index=0,
                message=ChatMessage(role="assistant", content=answer),
                finish_reason="stop"
            )
        ],
        usage=Usage(
            prompt_tokens=len(query) // 2,
            completion_tokens=len(answer) // 2,
            total_tokens=(len(query) + len(answer)) // 2
        )
    )

# ============== å¯åŠ¨äº‹ä»¶ ==============

@app.on_event("startup")
def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–æµè§ˆå™¨å’Œæ ‡ç­¾é¡µæ± """
    global browser, tab_pool
    
    print("=" * 50)
    print("ğŸš€ Pantheon API v0.4.0 (å¤šæ ‡ç­¾é¡µå¹¶è¡Œç‰ˆ)")
    print("=" * 50)
    
    try:
        co = ChromiumOptions()
        co.set_local_port(CHROME_PORT)
        co.set_argument('--no-sandbox')
        
        print(f"ğŸ”Œ è¿æ¥ Chrome (ç«¯å£ {CHROME_PORT})...")
        browser = ChromiumPage(addr_or_opts=co)
        print("âœ… æµè§ˆå™¨è¿æ¥æˆåŠŸ")
        
        # åˆå§‹åŒ–æ ‡ç­¾é¡µæ± 
        tab_pool = TabPoolManager(
            browser=browser,
            max_tabs_per_bot=3,  # æ¯ç§ Bot æœ€å¤š 3 ä¸ªå¹¶è¡Œæ ‡ç­¾é¡µ
            tab_timeout=300      # é—²ç½® 5 åˆ†é’Ÿåæ¸…ç†
        )
        
        print("\n" + "=" * 50)
        print("ğŸ“Œ æ”¯æŒå¹¶è¡Œè¯·æ±‚ï¼Œæ¯ç§æ¨¡å‹æœ€å¤š 3 ä¸ªå¹¶å‘")
        print("ğŸ“Œ æ¨¡å‹: kimi, deepseek, yuanbao, lmarena:<model>")
        print("ğŸ“Œ API: http://127.0.0.1:8000/docs")
        print("=" * 50 + "\n")
        
    except Exception as e:
        print(f"\nâŒ å¯åŠ¨å¤±è´¥: {e}")
        print(f'è¯·å…ˆå¯åŠ¨ Chrome: chrome --remote-debugging-port={CHROME_PORT}')
        sys.exit(1)


@app.on_event("shutdown")
def shutdown_event():
    """å…³é—­æ—¶æ¸…ç†èµ„æº"""
    global executor
    executor.shutdown(wait=False)
    print("ğŸ‘‹ æœåŠ¡å·²å…³é—­")

# ============== API è·¯ç”± ==============

@app.get("/")
def root():
    """æœåŠ¡çŠ¶æ€"""
    stats = tab_pool.get_stats() if tab_pool else {}
    return {
        "status": "running",
        "version": "0.4.0",
        "parallel": True,
        "tab_stats": stats,
        "models": ["kimi", "deepseek", "yuanbao", "lmarena"],
        "docs": "/docs"
    }


@app.get("/health")
def health():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "browser": browser is not None,
        "tab_pool": tab_pool is not None
    }


@app.get("/v1/models", response_model=ModelListResponse)
def list_models():
    """è·å–å¯ç”¨æ¨¡å‹"""
    now = int(time.time())
    return ModelListResponse(data=[
        ModelInfo(id="kimi", created=now, owned_by="moonshot"),
        ModelInfo(id="deepseek", created=now, owned_by="deepseek"),
        ModelInfo(id="yuanbao", created=now, owned_by="tencent"),
        ModelInfo(id="lmarena", created=now, owned_by="lmarena"),
        ModelInfo(id="lmarena:gpt-4o", created=now, owned_by="lmarena"),
        ModelInfo(id="lmarena:claude-opus-4", created=now, owned_by="lmarena"),
    ])


@app.get("/v1/pool/stats")
def pool_stats():
    """è·å–æ ‡ç­¾é¡µæ± çŠ¶æ€"""
    if not tab_pool:
        return {"error": "æ ‡ç­¾é¡µæ± æœªåˆå§‹åŒ–"}
    return tab_pool.get_stats()


@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
def chat_completions(
    request: ChatCompletionRequest,
    authorization: Optional[str] = Header(None)
):
    """
    OpenAI å…¼å®¹å¯¹è¯æ¥å£ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰
    
    æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ç‹¬ç«‹æ ‡ç­¾é¡µï¼Œæ”¯æŒå¤šè¯·æ±‚å¹¶è¡Œå¤„ç†
    """
    if request.stream:
        raise HTTPException(status_code=400, detail="æš‚ä¸æ”¯æŒæµå¼è¾“å‡º")
    
    # æ„å»ºæŸ¥è¯¢
    query = build_query(request.messages)
    if not query.strip():
        raise HTTPException(status_code=400, detail="æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º")
    
    # è§£ææ¨¡å‹å¹¶è·¯ç”±
    bot_type, specific_model = parse_model_name(request.model)
    
    if bot_type not in BOT_CLASSES:
        raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ¨¡å‹: {request.model}")
    
    print(f"[API] æ”¶åˆ°è¯·æ±‚: {request.model} -> {bot_type}")
    
    try:
        # åœ¨æ ‡ç­¾é¡µæ± ä¸­æ‰§è¡Œï¼ˆè‡ªåŠ¨åˆ†é…æ ‡ç­¾é¡µï¼‰
        result = execute_chat(bot_type, query, specific_model)
        return build_response(result)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/v1/pool/cleanup")
def cleanup_pool(background_tasks: BackgroundTasks):
    """æ‰‹åŠ¨æ¸…ç†é—²ç½®æ ‡ç­¾é¡µ"""
    if tab_pool:
        background_tasks.add_task(tab_pool.cleanup_idle_tabs)
        return {"message": "æ¸…ç†ä»»åŠ¡å·²æäº¤"}
    return {"error": "æ ‡ç­¾é¡µæ± æœªåˆå§‹åŒ–"}


# ============== ä¸»å…¥å£ ==============

if __name__ == "__main__":
    print("\nğŸ“Œ Pantheon API v0.4.0 - å¤šæ ‡ç­¾é¡µå¹¶è¡Œç‰ˆ")
    print("=" * 50)
    print("ç‰¹æ€§: æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ç‹¬ç«‹æ ‡ç­¾é¡µï¼Œæ”¯æŒå¹¶è¡Œå¤„ç†")
    print("=" * 50 + "\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)