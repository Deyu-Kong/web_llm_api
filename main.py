# main.py
"""
WebLLM API v0.5
é€šè¿‡ DrissionPage æ§åˆ¶æµè§ˆå™¨ï¼Œå°†ç½‘é¡µç‰ˆ LLM å°è£…ä¸º OpenAI å…¼å®¹ API
"""

import uvicorn
import time
import uuid
import sys
from fastapi import FastAPI, HTTPException, Header
from pydantic import BaseModel, Field
from typing import Optional, List, Literal
from DrissionPage import ChromiumPage, ChromiumOptions

from config import CHROME_PORT, CHROME_USER_DATA_DIR, DEFAULT_LMARENA_MODEL
from adapters import KimiBot, LMArenaBot, YuanbaoBot, DeepSeekBot

# ============== FastAPI åˆå§‹åŒ– ==============
app = FastAPI(
    title="WebLLM API",
    description="å°†ç½‘é¡µç‰ˆ LLM å°è£…ä¸º OpenAI å…¼å®¹ API",
    version="0.3.0"
)

# ============== å…¨å±€å˜é‡ ==============
browser = None
bots = {}  # ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ Bot å®ä¾‹

# ============== æ•°æ®æ¨¡å‹ ==============

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = Field(default=1.0, ge=0, le=2)
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False

class ChatCompletionChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: Literal["stop", "length"]

class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str
    object: Literal["chat.completion"] = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Usage

class ModelInfo(BaseModel):
    id: str
    object: Literal["model"] = "model"
    created: int
    owned_by: str

class ModelListResponse(BaseModel):
    object: Literal["list"] = "list"
    data: List[ModelInfo]

# ============== æ¨¡å‹è·¯ç”± ==============

# æ”¯æŒçš„æ¨¡å‹æ˜ å°„
MODEL_ALIASES = {
    "kimi": ("kimi", None),
    "deepseek": ("deepseek", None),
    "ds": ("deepseek", None),
    "yuanbao": ("yuanbao", None),
    "tencent": ("yuanbao", None),
    "å…ƒå®": ("yuanbao", None),
    "lmarena": ("lmarena", None),
}

def parse_model_name(model: str) -> tuple:
    """
    è§£ææ¨¡å‹åç§°
    è¿”å›: (bot_type, specific_model)
    """
    model = model.lower().strip()
    
    # æ£€æŸ¥ lmarena:xxx æ ¼å¼
    if model.startswith("lmarena:"):
        return ("lmarena", model.split(":", 1)[1])
    
    # æŸ¥æ‰¾åˆ«å
    if model in MODEL_ALIASES:
        return MODEL_ALIASES[model]
    
    # é»˜è®¤ä½œä¸º lmarena æ¨¡å‹
    return ("lmarena", model)


def call_bot(bot_type: str, query: str, specific_model: str = None) -> dict:
    """
    è°ƒç”¨æŒ‡å®šçš„ Bot
    è¿”å›: {"model": str, "thought": str, "answer": str}
    """
    if bot_type not in bots or bots[bot_type] is None:
        raise HTTPException(status_code=503, detail=f"{bot_type} æœºå™¨äººæœªåˆå§‹åŒ–")
    
    bot = bots[bot_type]
    bot.activate()
    
    # æ ¹æ® Bot ç±»å‹è°ƒç”¨
    if bot_type == "kimi":
        answer = bot.ask(query)
        if isinstance(answer, str) and answer.startswith("Error:"):
            raise HTTPException(status_code=500, detail=answer)
        return {"model": "kimi", "thought": "", "answer": answer}
    
    elif bot_type == "lmarena":
        result = bot.ask(query, model_name=specific_model)
        if result["answer"].startswith("Error:"):
            raise HTTPException(status_code=500, detail=result["answer"])
        model_name = f"lmarena:{specific_model}" if specific_model else "lmarena"
        return {"model": model_name, "thought": result.get("thought", ""), "answer": result["answer"]}
    
    else:  # deepseek, yuanbao
        result = bot.ask(query)
        if result["answer"].startswith("Error:"):
            raise HTTPException(status_code=500, detail=result["answer"])
        return {"model": bot_type, "thought": result.get("thought", ""), "answer": result["answer"]}



# ============== å¯åŠ¨äº‹ä»¶ ==============
@app.on_event("startup")
def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶è¿æ¥æµè§ˆå™¨å¹¶åˆå§‹åŒ–æ‰€æœ‰ Bot"""
    global browser, bots
    
    print("=" * 50)
    print("ğŸš€ Web-LLM API v0.3.0 å¯åŠ¨ä¸­...")
    print("=" * 50)
    
    try:
        # é…ç½®å¹¶è¿æ¥ Chrome
        co = ChromiumOptions()
        
        # è®¾ç½®è¿œç¨‹è°ƒè¯•ç«¯å£
        co.set_local_port(CHROME_PORT)
                
        # å…¶ä»–æœ‰ç”¨çš„é…ç½®
        co.set_argument('--no-sandbox')  # ç¦ç”¨æ²™ç›’æ¨¡å¼
        # co.set_argument('--disable-gpu')  # ç¦ç”¨ GPU åŠ é€Ÿï¼ˆå¯é€‰ï¼‰
        # co.headless(False)  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼ˆé»˜è®¤ï¼‰
        
        # è¿æ¥æˆ–å¯åŠ¨æµè§ˆå™¨
        print(f"ğŸ”Œ å°è¯•è¿æ¥åˆ°ç«¯å£ {CHROME_PORT}...")
        browser = ChromiumPage(addr_or_opts=co)
        
        print(f"âœ… æˆåŠŸè¿æ¥åˆ° Chrome (ç«¯å£: {CHROME_PORT})")
        
        # åˆå§‹åŒ–æ‰€æœ‰ Bot
        bots["kimi"] = KimiBot(browser)
        bots["lmarena"] = LMArenaBot(browser, model_name=DEFAULT_LMARENA_MODEL)
        bots["yuanbao"] = YuanbaoBot(browser)
        bots["deepseek"] = DeepSeekBot(browser)
        
        print("âœ… æ‰€æœ‰æœºå™¨äººåˆå§‹åŒ–å®Œæˆ")
        print("\n" + "=" * 50)
        print("ğŸ“Œ æ”¯æŒçš„æ¨¡å‹: kimi, deepseek, yuanbao, lmarena:<model>")
        print("ğŸ“Œ API æ–‡æ¡£: http://127.0.0.1:8000/docs")
        print("=" * 50 + "\n")
        
    except Exception as e:
        print(f"\nâŒ å¯åŠ¨å¤±è´¥: {e}")
        print(f"\nè¯·å…ˆå¯åŠ¨ Chrome è°ƒè¯•æ¨¡å¼:")
        print(f'  chrome --remote-debugging-port={CHROME_PORT} --user-data-dir="{CHROME_USER_DATA_DIR}"')
        sys.exit(1)


def build_query(messages: List[ChatMessage]) -> str:
    """å°†æ¶ˆæ¯åˆ—è¡¨æ„å»ºä¸ºæŸ¥è¯¢æ–‡æœ¬"""
    parts = []
    for msg in messages:
        if msg.role == "system":
            parts.append(f"[ç³»ç»ŸæŒ‡ä»¤] {msg.content}")
        elif msg.role == "user":
            parts.append(msg.content)
    return "\n".join(parts)


def build_response(result: dict, request_model: str) -> ChatCompletionResponse:
    """æ„å»º OpenAI æ ¼å¼çš„å“åº”"""
    answer = result["answer"]
    
    # ä¼°ç®— tokenï¼ˆç²—ç•¥ï¼‰
    prompt_tokens = len(result.get("query", "")) // 2
    completion_tokens = len(answer) // 2
    
    return ChatCompletionResponse(
        id=f"chatcmpl-{uuid.uuid4().hex[:12]}",
        created=int(time.time()),
        model=result["model"],
        choices=[
            ChatCompletionChoice(
                index=0,
                message=ChatMessage(role="assistant", content=answer),
                finish_reason="stop"
            )
        ],
        usage=Usage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=prompt_tokens + completion_tokens
        )
    )

# ============== API è·¯ç”± ==============

@app.get("/")
def root():
    """æœåŠ¡çŠ¶æ€"""
    return {
        "status": "running",
        "version": "0.3.0",
        "models": ["kimi", "deepseek", "yuanbao", "lmarena"],
        "docs": "/docs"
    }


@app.get("/health")
def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "browser": browser is not None}


@app.get("/v1/models", response_model=ModelListResponse)
def list_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    now = int(time.time())
    models = [
        ModelInfo(id="kimi", created=now, owned_by="moonshot"),
        ModelInfo(id="deepseek", created=now, owned_by="deepseek"),
        ModelInfo(id="yuanbao", created=now, owned_by="tencent"),
        ModelInfo(id="lmarena", created=now, owned_by="lmarena"),
        ModelInfo(id="lmarena:gpt-4o", created=now, owned_by="lmarena"),
        ModelInfo(id="lmarena:claude-opus-4", created=now, owned_by="lmarena"),
        ModelInfo(id="lmarena:gemini-2.5-pro", created=now, owned_by="lmarena"),
    ]
    return ModelListResponse(data=models)


@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
def chat_completions(
    request: ChatCompletionRequest,
    authorization: Optional[str] = Header(None)
):
    """
    OpenAI å…¼å®¹çš„å¯¹è¯æ¥å£
    
    æ”¯æŒçš„æ¨¡å‹:
    - kimi: Kimi
    - deepseek / ds: DeepSeek
    - yuanbao / tencent: è…¾è®¯å…ƒå®
    - lmarena: LMArena é»˜è®¤æ¨¡å‹
    - lmarena:<model>: LMArena æŒ‡å®šæ¨¡å‹
    """
    if request.stream:
        raise HTTPException(status_code=400, detail="æš‚ä¸æ”¯æŒæµå¼è¾“å‡º")
    
    # æ„å»ºæŸ¥è¯¢
    query = build_query(request.messages)
    if not query.strip():
        raise HTTPException(status_code=400, detail="æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º")
    
    # è§£ææ¨¡å‹å¹¶è·¯ç”±
    bot_type, specific_model = parse_model_name(request.model)
    print(f"[API] æ¨¡å‹: {request.model} -> {bot_type}, æŸ¥è¯¢: {query[:50]}...")
    
    # è°ƒç”¨ Botï¼ˆæ¯æ¬¡å¼€æ–°å¯¹è¯ï¼Œä¿æŒæ— çŠ¶æ€ï¼‰
    bot = bots.get(bot_type)
    if bot:
        bot.new_chat()
    
    result = call_bot(bot_type, query, specific_model)
    result["query"] = query
    
    print(f"[API] å®Œæˆï¼Œå›å¤é•¿åº¦: {len(result['answer'])} å­—ç¬¦")
    
    return build_response(result, request.model)


# ============== ä¸»å…¥å£ ==============

if __name__ == "__main__":
    print("\nğŸ“Œ ä½¿ç”¨æ–¹æ³•:")
    print("=" * 50)
    print("1. å¯åŠ¨ Chrome è°ƒè¯•æ¨¡å¼")
    print("2. åœ¨æµè§ˆå™¨ä¸­ç™»å½•å„ LLM å¹³å°")
    print("3. è¿è¡Œæœ¬ç¨‹åº")
    print("\nç¤ºä¾‹è°ƒç”¨:")
    print('  from openai import OpenAI')
    print('  client = OpenAI(base_url="http://127.0.0.1:8000/v1", api_key="x")')
    print('  client.chat.completions.create(model="kimi", messages=[...])')
    print("=" * 50 + "\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)